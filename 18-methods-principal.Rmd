# Principals of Differential Analysis methods

[@nearing2022microbiome] has showed that the multiple DA methods have different performances on microbiota (16s). Here, we integrated the its methods' part and our own understandings to give a draft whole picture for users who using DA of XMAS2. 


Microbiota data have the following characteristics:

1. **Compositional**: Mathematically, a data is defined as compositional, if it contains D multiple parts of nonnegative numbers whose sum is 1 or any constant-sum constraint. It can be formally stated as:

$$S^{D} = \left \{ X = [x_{1}, x_{2}, ..., x_{D}] | x_{i} > 0, i = 1, 2, ..., D; \sum^{D}_{i=1} x_{i} = K \right \}$$


2. **Over-Dispersed**: The variance are generally greater than the mean. There are several possible causes of overdispersion: correlated taxa counts, clustering of subjects, and within group heterogeneity. Negative Binomial Model addresses overdispersion by by explicitly modeling the correlated and sparse events via a latent variable.


3. **Sparsity**: Microbiome data typically is overdispersed and sparse with many zeros. When the number of zeros is excess than the standard distributions (e.g., normal, Poisson, binomial, NB, beta or gamma) can be readily fit, the data set is considered as ‘zero inflate.



## ALDEx2

ALDEx2 is from [@fernandes2014unifying]

### Vignette

[ANOVA-Like Differential Expression tool for high throughput sequencing data: ALDEx2](https://www.bioconductor.org/packages/devel/bioc/vignettes/ALDEx2/inst/doc/ALDEx2_vignette.html)

### Input Data Type

All the input data type should be **Raw Counts**:

1. 16s rRNA sequencing (amplicon sequencing); 

2. Metagenomic sequencing (absolute abundance);

3. RNA-seq.

### Input Key Arguments

1. **reads**: Raw counts matrix (Rows->Features; columns->Samples).

2. **conditions**: A vector of group labels for testing.

3. **mc.samples**: The number of Monte Carlo samples, default is 128.

4. **denom**: The methods for Geometric Mean calculation, default is "all".

5. **test**: The test to perform, default is "t.test".


### Procedures

1. Monte Carlo samples of Dirichlet distributions for each sample, using a uniform prior; (Sparsity)

2. Performing CLR (log-ratio) transformation  of each realization; (Compositional & Overdispersion)

3. Performing Wilcoxon tests on the transformed realizations. 

Finally, the function returned the expected Benjamini-Hochberg (BH) FDR-corrected p-value for each feature based on the results the different across Monte Carlo samples. **ALDEx2’s conservative nature is most likely due to its Monte Carlo Dirichlet sampling approach which down weights low abundance ASVs.**

### Key output

### Original Syntax

### XMAS2 Syntax

### Notes


## ANCOM-II

[ANCOM-II (version 2.1)](https://github.com/FrederickHuangLin/Microbiome-Review-Code-Archive/tree/master/scripts) :

1. `feature_table_pre_process` firstly identify outlier zeros and structural zeros; 

  * Outlier zeros, identified by finding outliers in the distribution of taxon counts within each sample grouping, were ignored during differential abundance analysis, and replaced with NA.
  
  * Structural zeros, taxa that were absent in one grouping but present in the other, were ignored during data analysis and automatically called as differential abundant.

2. additive log-ratios for transformation with a pseudo count of 1; 

3. Wilcoxon rank-sum tests for significance, and p-values were FDR-corrected using the BH method;

4. Detection threshold for Ajusted-pvalue per taxa, if the number (ratio) of Ajusted-pvalue reaches nominal significance would be called as DA taxa.


## corncob

1. Taxon count abundance fit to a **beta-binomial model**, using logit link functions for both the mean and overdispersion.

2. Wald tests for significance testing, and BH for FDR-corrected p-values.


## DESeq2

1. estimation of size factors, which are used to normalize library sizes in a model-based fashion;

2. estimation of dispersions from the negative binomial likelihood for each feature, and subsequent shrinkage of each dispersion estimate towards the parametric (default) trendline by empirical Bayes;

3. fitting each feature to the specified class groupings with negative binomial generalized linear models and performing hypothesis testing, for which we chose the default Wald test;

4. Finally, using the `results` function to obtain the resulting BH FDR-corrected p-values.


## edgeR

1. Add a pseudocount of 1 to the non-rarefied feature table and used the function `calcNormFactors` from the edgeR package to compute relative log expression normalization factors;

2. Negative binomial dispersion parameters were then estimated using the functions `estimateCommonDisp` followed by `estimateTagwiseDisp` to shrink feature-wise dispersion estimates through an empirical Bayes approach;

3. `exactTest` for negative binomial data to identify features that differ between the specified groups;

4. The resulting p-values were then corrected for multiple testing with the BH method with the function `topTags`.


## LEfSe

1. Normalizing the counts data using *total sum scaling*, which divides each feature count by the total library size;

2. Performing a Kruskal-Wallis (which in our two-group case reduces to the Wilcoxon rank-sum) hypothesis test to identify potential differentially abundant features;

3. Using differentially abundant features to perform linear discriminant analysis (LDA) of class labels on abundances to estimate the effect sizes for sig- nificant features.

4. Only features with scaled LDA analysis scores above the threshold (default: 2.0) were called as DA.


## limma voom

1. Normalizing table by the trimmed mean of M-values (TMM) or TMM with singleton pairing (TMMwsp) option via `calcNormFactors function` (highly sparse data); 

2. After normalization, using the limma R package function `voom` to convert normalized counts to log2-counts-per-million and assign precision weights to each observation based on the mean-variance trend;

3. Using the functions `lmFit`, `eBayes`, and `topTable` in the limma R package to fit weighted linear regression models, perform tests based on an empirical Bayes moderated t-statistic76 and obtain BH FDR-corrected p-values.


## MaAsLin2

1. using *arcsine square-root transformation (AST)* for transformation and *total sum scaling normalization (TSS)* for normalization; 

2. The function fit a linear model (without random effects chosen) to each feature’s transformed abundance on the specified sample grouping, tested significance using a Wald test, and output BH FDR-corrected p-values;


## metagenomeSeq

1. Converting the counts and sample information into newMRexperiment object by the function `newMRexperiment` from the metagenomeSeq R package;

2. Using `cumNormStat` and `cumNorm` to apply **cumulative sum-scaling normalization (CSS)**, which attempts to normalize sequence counts based on the lower-quartile abundance of features;

3. Using `fitFeatureModel` to fit normalized feature counts with zero-inflated log-normal models (with pseudo-counts of 1 added prior to log2 transformation) and perform empirical Bayes moderated t-tests, and `MRfulltable` to obtain BH FDR-corrected p-values.


## t-test

1. Applying *total sum scaling normalization (TSS)* or not to the feature table; 


2. Performing an unpaired Welch’s t-test for each feature to compare the specified groups;

3. Correcting the resulting p-values with the BH method.

## Wilcoxon test

1. CLR (after applying a pseudocount of 1) transforms or not to abundances; 

2. Performing Wilcoxon rank-sum tests for each feature to compare the specified sample groupings;

3. Correcting the resulting p-values with the BH method.


## RAIDA

**Approach for Identifying Differential Abundance (RAIDA)** - is a robust approach for identifying differentially abundant features in metagenomic samples across different conditions. It utilizes the ratio between features in a modified zero-inflated log-normal model.

1. Utilizing the ratios between the counts of features in each sample, eliminating possible problems associated with counts on different scales within and between conditions;

2. To fit ratios with zeros by EM algorithm, using a modified ZIL (zero-inflated log normal: sparsity);

3. Constructing a moderated t-statistics for the log ratio of each feature $y_{ij}$ using the estimated mean $\mu$ and variance $r^2$ and obtain $P$ values for the null hypotheses for all features;

4. Adjust $P$ values using a multiple testing correction method


## Summary

1. The most commonly assumed data distribution was the negative binomial distribution (DESeq2, edgeR, omnibus, mbzinb). 

> No data transformations were performed for negative binomial methods, or metagenomeSeq methods, to try and bring the data to normality as non-normality of data is taken into account in their statistical models. 


2. The remaining parametric methods (ALDEx2 t-test, t-test, limma-voom) all used statistical tests that assumed a Gaussian distribution of the data, therefore, transformations were needed before analysis, which here included a log transform of some kind.


3. Three methods (ALDEx2 Wilcoxon, ANCOM-II, LEfSe) were considered non-parametric (assumes no underlying distribution of data) as they used statistical tests that transformed data to ranks.


4. Two of the four negative binomial methods (DESeq2, edgeR) calculated scaling factors for each sample to account for uneven sequence count. 

5. Cumulative sum scaling (CSS) was used for both metagenomeSeq and MaAsLin2. Total sum scaling (TSS; also referred to as relative abundance) was performed for LEfSe, and for method that did not have a built-in normalization function (t-test) as this strategy is commonly used in the literature. Log-ratio based transformations were used for ALDEx2 methods, ANCOM, and, in addition to TSS, were applied to methods without a built-in normalization function.


## Advantages and disadvantages

1. limma-voom, edgeR, Wilcoxon (CLR), and LEfSe output a high number of significant ASVs on average, while ALDEx2 and ANCOM-II tended to identify only a relatively small number of ASVs as significant;

2. limma-voom, edgeR, Wilcoxon (CLR), and LEfSe methods have been previously found to exhibit a high FDR, in contrast, ANCOM appropriately controls the FDR;

3. We can clearly recommend that users avoid using edgeR (a tool primarily intended for RNA-seq data) as well as LEfSe (without p-value correction) for conducting DA testing with 16S rRNA gene data.

4. Users should also be aware that limma voom and the Wilcoxon (CLR) approaches may perform poorly on unfiltered data that is highly sparse.

5. More generally, we recommend that users employ several methods and focus on significant features identified by most tools. 


