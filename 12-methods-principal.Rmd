# Principals of Differential Analysis methods

[@nearing2022microbiome] has showed that the multiple DA methods have different performances on microbiota (16s). Here, we integrated the its methods' part and our own understandings to give a draft whole picture for users who using DA of XMAS2. 


Microbiota data have the following characteristics:

1. **Compositional**: Mathematically, a data is defined as compositional, if it contains D multiple parts of nonnegative numbers whose sum is 1 or any constant-sum constraint. It can be formally stated as:

$$S^{D} = \left \{ X = [x_{1}, x_{2}, ..., x_{D}] | x_{i} > 0, i = 1, 2, ..., D; \sum^{D}_{i=1} x_{i} = K \right \}$$


2. **Over-Dispersed**: The variance are generally greater than the mean. There are several possible causes of overdispersion: correlated taxa counts, clustering of subjects, and within group heterogeneity. Negative Binomial Model addresses overdispersion by by explicitly modeling the correlated and sparse events via a latent variable.


3. **Sparsity**: Microbiome data typically is overdispersed and sparse with many zeros. When the number of zeros is excess than the standard distributions (e.g., normal, Poisson, binomial, NB, beta or gamma) can be readily fit, the data set is considered as ‘zero inflate.



## ALDEx2

ALDEx2 R package use the main *aldex*.

1. Monte Carlo samples of Dirichlet distributions for each sample, using a uniform prior; (Sparsity)

2. Performing CLR (log-ratio) transformation  of each realization; (Compositional & Overdispersion)

3. Performing Wilcoxon tests on the transformed realizations. 

Finally, the function returned the expected Benjamini-Hochberg (BH) FDR-corrected p-value for each feature based on the results the different across Monte Carlo samples. **ALDEx2’s conservative nature is most likely due to its Monte Carlo Dirichlet sampling approach which down weights low abundance ASVs.**

## ANCOM-II

[ANCOM-II (version 2.1)](https://github.com/FrederickHuangLin/Microbiome-Review-Code-Archive/tree/master/scripts) :

1. `feature_table_pre_process` firstly identify outlier zeros and structural zeros; 

  * Outlier zeros, identified by finding outliers in the distribution of taxon counts within each sample grouping, were ignored during differential abundance analysis, and replaced with NA.
  
  * Structural zeros, taxa that were absent in one grouping but present in the other, were ignored during data analysis and automatically called as differential abundant.

2. additive log-ratios for transformation with a pseudo count of 1; 

3. Wilcoxon rank-sum tests for significance, and p-values were FDR-corrected using the BH method;

4. Detection threshold for Ajusted-pvalue per taxa, if the number (ratio) of Ajusted-pvalue reaches nominal significance would be called as DA taxa.


## corncob

1. Taxon count abundance fit to a **beta-binomial model**, using logit link functions for both the mean and overdispersion.

2. Wald tests for significance testing, and BH for FDR-corrected p-values.


## DESeq2

1. estimation of size factors, which are used to normalize library sizes in a model-based fashion;

2. estimation of dispersions from the negative binomial likelihood for each feature, and subsequent shrinkage of each dispersion estimate towards the parametric (default) trendline by empirical Bayes;

3. fitting each feature to the specified class groupings with negative binomial generalized linear models and performing hypothesis testing, for which we chose the default Wald test;

4. Finally, using the `results` function to obtain the resulting BH FDR-corrected p-values.


## edgeR

1. Add a pseudocount of 1 to the non-rarefied feature table and used the function `calcNormFactors` from the edgeR package to compute relative log expression normalization factors;

2. Negative binomial dispersion parameters were then estimated using the functions `estimateCommonDisp` followed by `estimateTagwiseDisp` to shrink feature-wise dispersion estimates through an empirical Bayes approach;

3. `exactTest` for negative binomial data to identify features that differ between the specified groups;

4. The resulting p-values were then corrected for multiple testing with the BH method with the function `topTags`.


## LEfSe

1. Normalizing the counts data using *total sum scaling*, which divides each feature count by the total library size;

2. Performing a Kruskal-Wallis (which in our two-group case reduces to the Wilcoxon rank-sum) hypothesis test to identify potential differentially abundant features;

3. Using differentially abundant features to perform linear discriminant analysis (LDA) of class labels on abundances to estimate the effect sizes for sig- nificant features.

4. Only features with scaled LDA analysis scores above the threshold (default: 2.0) were called as DA.


## limma voom

1. Normalizing table by the trimmed mean of M-values (TMM) or TMM with singleton pairing (TMMwsp) option via `calcNormFactors function` (highly sparse data); 

2. After normalization, using the limma R package function `voom` to convert normalized counts to log2-counts-per-million and assign precision weights to each observation based on the mean-variance trend;

3. Using the functions `lmFit`, `eBayes`, and `topTable` in the limma R package to fit weighted linear regression models, perform tests based on an empirical Bayes moderated t-statistic76 and obtain BH FDR-corrected p-values.


## MaAsLin2

1. using *arcsine square-root transformation (AST)* for transformation and *total sum scaling normalization (TSS)* for normalization; 

2. The function fit a linear model (without random effects chosen) to each feature’s transformed abundance on the specified sample grouping, tested significance using a Wald test, and output BH FDR- corrected p-values;


## metagenomeSeq

1. Converting the counts and sample information into newMRexperiment object by the function `newMRexperiment` from the metagenomeSeq R package;

2. Using `cumNormStat` and `cumNorm` to apply **cumulative sum-scaling normalization (CSS)**, which attempts to normalize sequence counts based on the lower-quartile abundance of features;

3. Using `fitFeatureModel` to fit normalized feature counts with zero-inflated log-normal models (with pseudo-counts of 1 added prior to log2 transformation) and perform empirical Bayes moderated t-tests, and `MRfulltable` to obtain BH FDR-corrected p-values.


## t-test

1. Applying *total sum scaling normalization (TSS)* or not to the feature table; 


2. Performing an unpaired Welch’s t-test for each feature to compare the specified groups;

3. Correcting the resulting p-values with the BH method.

## Wilcoxon test

1. CLR (after applying a pseudocount of 1) transforms or not to abundances; 

2. Performing Wilcoxon rank-sum tests for each feature to compare the specified sample groupings;

3. Correcting the resulting p-values with the BH method.


